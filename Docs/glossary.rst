Glossary
********

.. Admonition::
    For any given algorithm entry, make sure you provide a simple, pseudocode implementation as well.




.. a:

ant colony optimization (ACO)
=============================
An optimization algorithm inspired by the swarm intelligence of social ants using pheremone as a chemical messenger. First published in Marco Dorigo's phD thesis in 1992.


.. b:

bat algorithm (BA)
==================
**EXPAND** A continuous optimization algorithm based on the echolocation behavior of microbats. (2010 Yang, XS)

Bees
====
Algorithms inspired by the behavior of bees

bees algorithm
--------------
**TODO** 2005 D.T. Pham

artificial bee colony (ABC)
---------------------------
**TODO** 2005 Karaboga


honeybee algorithm
------------------
**TODO** 2004 Sunil Nakrani

virtual bee algorithm
---------------------
**TODO** 2005 Xin-She Yang


.. c:

cuckoo search (CS)
==================
**TODO** 2009 Xin-She Yang

.. d:

differential evolution (DE)
===========================
**EXPAND**. A vector-based evolutionary algorithm published in 1996. 

.. e:

.. f:

firefly algorithm (FA)
======================
**TODO** 2007 Xin-She Yang

flower pollination algorithm
============================
**TODO** 2012 Xin-She Yang

.. g:

.. h:

harmony search (HS)
===================
**EXPAND** 2001 Zong Woo Geem

honeybee algorithm
==================
**see bees**

virtual bee algorithm
---------------------

.. i:

.. j:

.. k:

.. l:

.. m:

.. n:

No Free Lunch Theorem
=====================
Published in 1997, the theorem states if algorithm A performs better than algorithm B for some optimization functions, then B will outperform A for other functions. ie, if averaged over all possible function space, both algorithms A and B will perform equally well. Alternatively, no universally better algorithms exist. [Yang2014]_

By NFLT, there is no universally better optimization algorithm. However, research can be devoted to finding the most efficient algorithm for a given set of problems.

.. o:

.. p:

particle swarm optimization (PSO)
=================================
Optimization algorithm inspired by swarm intelligence of fish and birds and even by human behavior. The multiple agents, called *particles*, swarm around the search space, starting from some initial random guess. The swarm communicates the current best guess and shares the global best so as to focus on the quality solutions.

Since it's publication in 1995, there have been about 20 different variants of PSO techniques, which have been applied to almost all areas of challenging optimization problems, and there is strong evidence that PSO is better than traditional search algorithms and even better than GA for many types of problems. [Yang2014]_

PSO variants
------------



.. q:

.. r:

.. s:

simulated annealing (SA)
========================
Metaheuristic inspired by the annealing process of metals. It is a trajectory-based search algorithm, starting with an initial guess solution at a high temperature and gradually cooling down the system. A move or new solution is accepted if it is better; otherwise, it is accepted with a probability, allowing it to escape any local optima. It is then expected that if the system is cooled down slowly enough, the global optimal solution can be reached. [Yang2014]_

Swarm Intelligence (SI)
=======================
**TODO** Expand (this is a major domain/umbrella for many NIH)

.. t:

.. u:

.. v:

.. w:

.. x:

.. y:

.. z:


References
**********


.. [Yang2014] Yang, Xin-She. (2014). Nature-Inspired Optimization Algorithms. `Full-text PDF <https://www.researchgate.net/publication/263171713_Nature-Inspired_Optimization_Algorithms>`_


